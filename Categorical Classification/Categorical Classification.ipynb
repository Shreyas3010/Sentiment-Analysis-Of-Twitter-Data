{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is for Categorical Classification between various classes.\n",
    "\n",
    "\n",
    "LABELS :-\n",
    "\n",
    "0 - ENTERTAINMENT\n",
    "1 - POLITICS\n",
    "2 - SPORTS\n",
    "3 - BUSINESS\n",
    "4 - EDUCATION\n",
    "\n",
    "\n",
    "\n",
    "#### There are total 3 Cells in this notebook.\n",
    "1) Main Cell for machine learning model training and dataset creation, etc.\n",
    "2) Creates a classification report of the model using the testing set.\n",
    "3) This cell contains all the important functions necessary\n",
    "\n",
    "\n",
    "#### Order of running of cells...\n",
    "First you need to run the 3rd cell for defining the functions. Then run the first cell to start the model training. Now run the 3rd cell to generate a classification report.\n",
    "\n",
    "\n",
    "\n",
    "#### NOTE:- You need to change the required paths to various resourcces given in the first cell to run the code properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "import h5py\n",
    "#import utility_functions as uf\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_data_all(data_dir, all_data_path,pred_path, gloveFile, first_run, load_all):\n",
    "    \n",
    "    # Load embeddings for the filtered glove list\n",
    "    if load_all == True:\n",
    "        weight_matrix, word_idx = load_embeddings(gloveFile)\n",
    "    else:\n",
    "        weight_matrix, word_idx = load_embeddings(filtered_glove_path)\n",
    "\n",
    "    len(word_idx)\n",
    "    len(weight_matrix)\n",
    "\n",
    "    #%%\n",
    "    # create test, validation and trainng data\n",
    "    all_data = read_data(all_data_path)\n",
    "    #print(all_data)\n",
    "\n",
    "    # Shuffling Dataset\n",
    "    \n",
    "    print('Shuffling Dataset...')\n",
    "    all_data = shuffle(all_data)\n",
    "    print('Shuffling Finished...')\n",
    "    all_data = all_data\n",
    "    print(all_data)\n",
    "\n",
    "    train_data, test_data, dev_data = training_data_split(all_data, 0.8, data_dir)\n",
    "\n",
    "    train_data = train_data.reset_index()\n",
    "    dev_data = dev_data.reset_index()\n",
    "    test_data = test_data.reset_index()\n",
    "\n",
    "    #%%\n",
    "    # inputs from dl_sentiment that are hard coded but need to be automated\n",
    "    maxSeqLength, avg_words, sequence_length = maxSeqLen(all_data)\n",
    "    #numClasses = 10\n",
    "    numClasses = 5\n",
    "\n",
    "    #%%\n",
    "\n",
    "     # load Training data matrix\n",
    "    print('Load training data matrix start...')\n",
    "    train_x = tf_data_pipeline_nltk(train_data, word_idx, weight_matrix, maxSeqLength)\n",
    "    test_x = tf_data_pipeline_nltk(test_data, word_idx, weight_matrix, maxSeqLength)\n",
    "    val_x = tf_data_pipeline_nltk(dev_data, word_idx, weight_matrix, maxSeqLength)\n",
    "    print('Load training data matrix end...')\n",
    "\n",
    "    #%%\n",
    "    # load labels data matrix\n",
    "    print('Load labels data matrix start...')\n",
    "    train_y = labels_matrix(train_data)\n",
    "    val_y = labels_matrix(dev_data)\n",
    "    test_y = labels_matrix(test_data)\n",
    "    print('Load labels data matrix end...')\n",
    "\n",
    "\n",
    "     #%%\n",
    "\n",
    "    # summarize size\n",
    "    print(\"Training data: \")\n",
    "    print(train_x.shape)\n",
    "    print(train_y.shape)\n",
    "\n",
    "    # Summarize number of classes\n",
    "    print(\"Classes: \")\n",
    "    print(np.unique(train_y.shape[1]))\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y, weight_matrix, word_idx\n",
    "\n",
    "def create_model_rnn(weight_matrix, max_words, EMBEDDING_DIM):\n",
    "\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(weight_matrix), EMBEDDING_DIM, weights=[weight_matrix], input_length=max_words, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.20))\n",
    "    #model.add(Dense(1024, activation='relu'))\n",
    "    #model.add(Dropout(0.20))\n",
    "    #model.add(Dense(512, activation='relu'))\n",
    "    #model.add(Dropout(0.20))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model,train_x, train_y, test_x, test_y, val_x, val_y, batch_size, path, epochs) :\n",
    "\n",
    "    # save the best model and early stopping\n",
    "    saveBestModel = keras.callbacks.ModelCheckpoint(path+'/categorical/model/best_model.hdf5', monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs,validation_data=(val_x, val_y), callbacks=[saveBestModel, earlyStopping])\n",
    "    # Final evaluation of the model\n",
    "    score, acc = model.evaluate(test_x, test_y, batch_size=batch_size)\n",
    "\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "    return model\n",
    "\n",
    "def live_test(trained_model, data, word_idx):\n",
    "\n",
    "    #data = \"Pass the salt\"\n",
    "    #data_sample_list = data.split()\n",
    "    live_list = []\n",
    "    live_list_np = np.zeros((139,1))\n",
    "    # split the sentence into its words and remove any punctuations.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data_sample_list = tokenizer.tokenize(data)\n",
    "\n",
    "    labels = np.array(['0','1','2','3','4'], dtype = \"int\")\n",
    "    #word_idx['I']\n",
    "    # get index for the live stage\n",
    "    data_index = np.array([word_idx[word.lower()] if word.lower() in word_idx else 0 for word in data_sample_list])\n",
    "    data_index_np = np.array(data_index)\n",
    "    #print(data_index_np)\n",
    "\n",
    "    # padded with zeros of length 56 i.e maximum length\n",
    "    padded_array = np.zeros(139) # use the def maxSeqLen(training_data) function to detemine the padding length for your data\n",
    "    padded_array[:data_index_np.shape[0]] = data_index_np\n",
    "    data_index_np_pad = padded_array.astype(int)\n",
    "    live_list.append(data_index_np_pad)\n",
    "    live_list_np = np.asarray(live_list)\n",
    "    type(live_list_np)\n",
    "\n",
    "    # get score from the model\n",
    "    score = trained_model.predict_classes(live_list_np, batch_size=1, verbose=0)\n",
    "    return score\n",
    "\n",
    "    '''single_score = np.round(np.argmax(score)/10, decimals=2) # maximum of the array i.e single band\n",
    "\n",
    "    # weighted score of top 3 bands\n",
    "    top_3_index = np.argsort(score)[0][-3:]\n",
    "    top_3_scores = score[0][top_3_index]\n",
    "    top_3_weights = top_3_scores/np.sum(top_3_scores)\n",
    "    single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)'''\n",
    "\n",
    "    #print (single_score)\n",
    "    #return single_score_dot\n",
    "\n",
    "\n",
    "max_words = 56 # max no of words in your training data\n",
    "batch_size = 500 # batch size for training\n",
    "EMBEDDING_DIM = 100 # size of the word embeddings\n",
    "epochs = 25\n",
    "train_flag = False # set True if in training mode else False if in prediction mode\n",
    "first_time = True\n",
    "path = 'drive/My Drive/deepsentiment'\n",
    "\n",
    "if train_flag:\n",
    "    # create training, validataion and test data sets\n",
    "    # load the dataset\n",
    "    path = 'drive/My Drive/deepsentiment'\n",
    "    data_dir = path+'/Data'\n",
    "    all_data_path = path+'/Data/'\n",
    "    pred_path = path+'/Data/output_model/test_pred.csv'\n",
    "    gloveFile = path+'/Data/glove/glove_twitter_27B_100d.txt'\n",
    "    first_run = False\n",
    "    load_all = True\n",
    "\n",
    "    train_x, train_y, test_x, test_y, val_x, val_y, weight_matrix, word_idx = load_data_all(data_dir, all_data_path,pred_path, gloveFile, first_run, load_all)\n",
    "    # create model strucutre\n",
    "    model = create_model_rnn(weight_matrix, train_x.shape[1], EMBEDDING_DIM)\n",
    "\n",
    "    # train the model\n",
    "    trained_model =train_model(model,train_x, train_y, test_x, test_y, val_x, val_y, batch_size, path, epochs)   # run model live\n",
    "\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    #model.save_weights(path+\"/categorical/model/best_model.hdf5\")\n",
    "    model.save(path+\"/categorical/model/best_model.hdf5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "else:\n",
    "    if first_time:\n",
    "        \n",
    "        ## Path to load GloVe embeddings\n",
    "        gloveFile = path+'/Data/glove/glove_twitter_27B_100d.txt'\n",
    "        weight_matrix, word_idx = load_embeddings(gloveFile)\n",
    "        \n",
    "        ## Path to load a saved model for testing\n",
    "        model_path = path +'/categorical/model/best_model_deep_4L.h5'\n",
    "        trained_model = trained_model(model_path)\n",
    "        trained_model.summary()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "            Insert your code here for appropriate testing.\n",
    "            Sample Code:- \n",
    "            \n",
    "            text = \"Hello this is testing statement.\"\n",
    "            result = live_test(loaded_model,text, word_idx)\n",
    "            print(result)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #loaded_model = trained_model\n",
    "    #data_sample = \"books study classroom class tuition\"\n",
    "    #live_test(loaded_model,data_sample, word_idx)\n",
    "    #result = live_test(loaded_model,data_sample, word_idx)\n",
    "    #print (result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "#print(test_y)\n",
    "\n",
    "actual=[]\n",
    "for i in range(len(test_y)):\n",
    "  actual.append(np.where(test_y[i] == np.max(test_y[i]))[0][0])\n",
    "\n",
    "predicted = trained_model.predict_classes(test_x)\n",
    "\n",
    "print(classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This cell is for the necessary functions like loading dataset, glove embeddings and preparing training data.\n",
    "\n",
    "Note:- Head over to the function \"read_data()\" to change the path for training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import codecs\n",
    "import os\n",
    "import progressbar as pb\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "#%%\n",
    "################################### Paths to Data ########################################################################\n",
    "\n",
    "path = '/Data/'\n",
    "gloveFile = '/Data/glove/glove_6B_300d.txt' #'/Users/prajwalshreyas/Desktop/Singularity/Topic modelling/Glove/glove.twitter.27B/glove.twitter.27B.25d.txt'\n",
    "vocab_path = '/Data/glove/vocab_glove.csv'\n",
    "\n",
    "#Split Data path\n",
    "train_data_path ='/Data/TrainingData/train.csv'\n",
    "val_data_path ='/Data/TrainingData/val.csv'\n",
    "test_data_path ='/Data/TrainingData/test.csv'\n",
    "\n",
    "sent_matrix_path ='/Data/inputs_model/sentence_matrix.csv'\n",
    "sent_matrix_path_val ='/Data/inputs_model/sentence_matrix_val.csv'\n",
    "sent_matrix_path_test ='/Data/inputs_model/sentence_matrix_test.csv'\n",
    "sequence_len_path = '/Data/inputs_model/sequence_length.csv'\n",
    "sequence_len_val_path = '/Data/inputs_model/sequence_length_val.csv'\n",
    "sequence_len_test_path = '/Data/inputs_model/sequence_length_test.csv'\n",
    "wordVectors_path = '/Data/inputs_model/wordVectors.csv'\n",
    "#%%#\n",
    "\n",
    "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Filtered Vocabulary from Glove document >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def filter_glove(full_glove_path, data_dir):\n",
    "  vocab = set()\n",
    "  sentence_path = os.path.join(data_dir,'SOStr.txt')\n",
    "  filtered_glove_path = os.path.join(data_dir, 'filtered_glove.txt')\n",
    "  # Download the full set of unlabeled sentences separated by '|'.\n",
    "  #sentence_path, = download_and_unzip(\n",
    "    #'http://nlp.stanford.edu/~socherr/', 'stanfordSentimentTreebank.zip',\n",
    "    #'stanfordSentimentTreebank/SOStr.txt')\n",
    "  with codecs.open(sentence_path, encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "      # Drop the trailing newline and strip backslashes. Split into words.\n",
    "      vocab.update(line.strip().replace('\\\\', '').split('|'))\n",
    "  nread = 0\n",
    "  nwrote = 0\n",
    "  with codecs.open(full_glove_path, encoding='latin-1') as f:\n",
    "    with codecs.open(filtered_glove_path, 'w', encoding='latin-1') as out:\n",
    "      for line in f:\n",
    "        nread += 1\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        if line.split(u' ', 1)[0] in vocab:\n",
    "          out.write(line + '\\n')\n",
    "          nwrote += 1\n",
    "  print('read %s lines, wrote %s' % (nread, nwrote))\n",
    "#%%#\n",
    "\n",
    "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Filtered Vocabulary from live cases >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "\n",
    "\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< load embeddings >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "def load_embeddings(embedding_path):\n",
    "  \"\"\"Loads embedings, returns weight matrix and dict from words to indices.\"\"\"\n",
    "  print('loading word embeddings from %s' % embedding_path)\n",
    "  weight_vectors = []\n",
    "  word_idx = {}\n",
    "  i=0\n",
    "  with codecs.open(embedding_path, encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "      try:\n",
    "        word, vec = line.split(u' ', 1)\n",
    "        word_idx[word] = len(weight_vectors)\n",
    "        weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "  print('For Loop Complete!!')\n",
    "  # Annoying implementation detail; '(' and ')' are replaced by '-LRB-' and\n",
    "  # '-RRB-' respectively in the parse-trees.\n",
    "  word_idx[u'-LRB-'] = word_idx.pop(u'(')\n",
    "  word_idx[u'-RRB-'] = word_idx.pop(u')')\n",
    "  # Random embedding vector for unknown words.\n",
    "  weight_vectors.append(np.random.uniform(\n",
    "      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "  print('Word Embeddings Loaded!')\n",
    "  return np.stack(weight_vectors), word_idx\n",
    "\n",
    "\n",
    "# Combine and split the data into train and test\n",
    "def read_data(path):\n",
    "    print('read_data start...')\n",
    "    '''\n",
    "    # read dictionary into df\n",
    "    df_data_sentence = pd.read_table(path + 'dictionary.txt')\n",
    "    df_data_sentence_processed = df_data_sentence['Phrase|Index'].str.split('|', expand=True)\n",
    "    df_data_sentence_processed = df_data_sentence_processed.rename(columns={0: 'Phrase', 1: 'phrase_ids'})\n",
    "\n",
    "    # read sentiment labels into df\n",
    "    df_data_sentiment = pd.read_table(path + 'sentiment_labels.txt')\n",
    "    df_data_sentiment_processed = df_data_sentiment['phrase ids|sentiment values'].str.split('|', expand=True)\n",
    "    df_data_sentiment_processed = df_data_sentiment_processed.rename(columns={0: 'phrase_ids', 1: 'sentiment_values'})\n",
    "\n",
    "\n",
    "    #combine data frames containing sentence and sentiment\n",
    "    df_processed_all = df_data_sentence_processed.merge(df_data_sentiment_processed, how='inner', on='phrase_ids')\n",
    "    '''\n",
    "\n",
    "    '''df_all = pd.read_table(path + 'glove_processed_data.txt', error_bad_lines=False, encoding='latin-1')\n",
    "    df_processed_all = df_all['index|polarity|tweet'].str.split('|', expand=True)\n",
    "    df_processed_all = df_processed_all.rename(columns={0: 'phrase_ids', 1: 'sentiment_values', 2: 'Phrase'})'''\n",
    "\n",
    "    df_processed_all = pd.read_csv(path + 'glove_processed_data_categorical.csv', sep=\"|\")\n",
    "    df_processed_all['polarity'] = df_processed_all['polarity'].astype(str)\n",
    "    #df_processed_all = df_processed_all[~df_processed_all[\"polarity\"].str.contains(\"5\")]\n",
    "    #df_processed_all['polarity'] = df_processed_all['polarity'].astype(int)\n",
    "    Counter(df_processed_all['polarity'])\n",
    "\n",
    "    print('read_data end...')\n",
    "\n",
    "    return df_processed_all\n",
    "\n",
    "def training_data_split(all_data, spitPercent, data_dir):\n",
    "    print('data_split start...')\n",
    "\n",
    "    msk = np.random.rand(len(all_data)) < spitPercent\n",
    "    train_only = all_data[msk]\n",
    "    test_and_dev = all_data[~msk]\n",
    "\n",
    "\n",
    "    msk_test = np.random.rand(len(test_and_dev)) <0.5\n",
    "    test_only = test_and_dev[msk_test]\n",
    "    dev_only = test_and_dev[~msk_test]\n",
    "\n",
    "    dev_only.to_csv(os.path.join(data_dir, 'TrainingData/dev.csv'))\n",
    "    test_only.to_csv(os.path.join(data_dir, 'TrainingData/test.csv'))\n",
    "    train_only.to_csv(os.path.join(data_dir, 'TrainingData/train.csv'))\n",
    "\n",
    "    print('data_split_end...')\n",
    "\n",
    "    return train_only, test_only, dev_only\n",
    "#%%\n",
    "################################### Glove Vector  ########################################################################\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',encoding='latin-1')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        try:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = [float(val) for val in splitLine[1:]]\n",
    "            model[word] = embedding\n",
    "        except:\n",
    "            print (word)\n",
    "            continue\n",
    "\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "#%%\n",
    "\n",
    "\n",
    "#%%\n",
    "################################### Create Vocab subset GLove vectors ########################################################################\n",
    "\n",
    "def word_vec_index(training_data, glove_model):\n",
    "\n",
    "    sentences = training_data['tweet'] # get the phrases as a df series\n",
    "    #sentences = sentences[0:100]\n",
    "    sentences_concat = sentences.str.cat(sep=' ')\n",
    "    sentence_words = re.findall(r'\\S+', sentences_concat)\n",
    "    sentence_words_lwr = [x.lower() for x in sentence_words]\n",
    "    subdict = {word: glove_model[word] for word in glove_model.keys() & sentence_words_lwr}\n",
    "\n",
    "    vocab_df = pd.DataFrame(subdict)\n",
    "    vocab_df.to_csv(vocab_path)\n",
    "    return vocab_df\n",
    "#%%\n",
    "################################### Convertdf to list ########################################################################\n",
    "def word_list(vocab_df):\n",
    "\n",
    "    wordVectors = vocab_df.values.T.tolist()\n",
    "    wordVectors_np = np.array(wordVectors)\n",
    "    wordList = list(vocab_df.columns.values)\n",
    "\n",
    "    return wordList, wordVectors_np\n",
    " #%%\n",
    "################################### tensorflow data pipeline ########################################################################\n",
    "\n",
    "\n",
    "def maxSeqLen(training_data):\n",
    "    print('maxSeqlen start...')\n",
    "\n",
    "    total_words = 0\n",
    "    sequence_length = []\n",
    "    idx = 0\n",
    "    i=0\n",
    "    bar = pb.ProgressBar(maxval=1600000, \\\n",
    "    widgets=[pb.Bar('=', '[', ']'), ' ', pb.Percentage()])\n",
    "    bar.start()\n",
    "    for index, row in training_data.iterrows():\n",
    "\n",
    "        sentence = (row['tweet'])\n",
    "        sentence_words = sentence.split()\n",
    "        len_sentence_words = len(sentence_words)\n",
    "        total_words = total_words + len_sentence_words\n",
    "\n",
    "        # get the length of the sequence of each training data\n",
    "        sequence_length.append(len_sentence_words)\n",
    "\n",
    "        if idx == 0:\n",
    "            max_seq_len = len_sentence_words\n",
    "\n",
    "\n",
    "        if len_sentence_words > max_seq_len:\n",
    "            max_seq_len = len_sentence_words\n",
    "            print(sentence_words)\n",
    "        idx = idx + 1\n",
    "        bar.update(i+1)\n",
    "        i+=1\n",
    "        \n",
    "\n",
    "    bar.finish()\n",
    "    avg_words = total_words/index\n",
    "\n",
    "    # convert to numpy array\n",
    "    sequence_length_np = np.asarray(sequence_length)\n",
    "\n",
    "    print('maxseqlen end...')\n",
    "\n",
    "    return max_seq_len, avg_words, sequence_length_np\n",
    "\n",
    "  #%%\n",
    "def tf_data_pipeline(data, word_idx, weight_matrix, max_seq_len):\n",
    "\n",
    "    #training_data = training_data[0:50]\n",
    "\n",
    "    maxSeqLength = max_seq_len #Maximum length of sentence\n",
    "    no_rows = len(data)\n",
    "    ids = np.zeros((no_rows, maxSeqLength), dtype='int32')\n",
    "    # conver keys in dict to lower case\n",
    "    word_idx_lwr =  {k.lower(): v for k, v in word_idx.items()}\n",
    "    idx = 0\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "\n",
    "        sentence = (row['tweet'])\n",
    "        sentence_words = sentence.split(' ')\n",
    "\n",
    "        i = 0\n",
    "        for word in sentence_words:\n",
    "            #print(index)\n",
    "            word_lwr = word.lower()\n",
    "            try:\n",
    "                #print (word_lwr)\n",
    "                ids[idx][i] =  word_idx_lwr[word_lwr]\n",
    "\n",
    "            except Exception as e:\n",
    "                #print (e)\n",
    "                #print (word)\n",
    "                if str(e) == word:\n",
    "                    ids[idx][i] = 0\n",
    "                continue\n",
    "            i = i + 1\n",
    "        idx = idx + 1\n",
    "    return ids\n",
    "\n",
    "  #%%\n",
    "# create labels matrix for the rnn\n",
    "\n",
    "\n",
    "def tf_data_pipeline_nltk(data, word_idx, weight_matrix, max_seq_len):\n",
    "\n",
    "    #training_data = training_data[0:50]\n",
    "\n",
    "    maxSeqLength = max_seq_len #Maximum length of sentence\n",
    "    no_rows = len(data)\n",
    "    ids = np.zeros((no_rows, maxSeqLength), dtype='int32')\n",
    "    # conver keys in dict to lower case\n",
    "    word_idx_lwr =  {k.lower(): v for k, v in word_idx.items()}\n",
    "    idx = 0\n",
    "    j=0\n",
    "    bar = pb.ProgressBar(maxval=no_rows, \\\n",
    "    widgets=[pb.Bar('=', '[', ']'), ' ', pb.Percentage()])\n",
    "    bar.start()\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "\n",
    "        sentence = (row['tweet'])\n",
    "        #print (sentence)\n",
    "        #tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        #tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "        #sentence_words = tokenizer.tokenize(sentence)\n",
    "        sentence_words = sentence.split()\n",
    "        #print (sentence_words)\n",
    "        i = 0\n",
    "        for word in sentence_words:\n",
    "            #print(index)\n",
    "            word_lwr = word.lower()\n",
    "            try:\n",
    "                #print (word_lwr)\n",
    "                ids[idx][i] =  word_idx_lwr[word_lwr]\n",
    "\n",
    "            except Exception as e:\n",
    "                #print (e)\n",
    "                #print(word)\n",
    "                if str(e) == word:\n",
    "                    ids[idx][i] = 0\n",
    "                continue\n",
    "            i = i + 1\n",
    "        idx = idx + 1\n",
    "        bar.update(j+1)\n",
    "        j+=1\n",
    "    \n",
    "    bar.finish()\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "def labels_matrix(data):\n",
    "\n",
    "    labels = data['polarity']\n",
    "\n",
    "    #lables_float = labels.astype(float)\n",
    "    lables_int = labels.astype(int)\n",
    "\n",
    "    #cats = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    cats = ['0','1','2','3','4']\n",
    "    #labels_mult = (lables_float * 10).astype(int)\n",
    "    labels_mult = lables_int\n",
    "    dummies = pd.get_dummies(labels_mult, prefix='', prefix_sep='')\n",
    "    dummies = dummies.T.reindex(cats).T.fillna(0)\n",
    "    #print(dummies)\n",
    "    labels_matrix = dummies.to_numpy()\n",
    "\n",
    "    return labels_matrix\n",
    "\n",
    "\n",
    "def labels_matrix_unmod(data):\n",
    "\n",
    "    labels = data['polarity']\n",
    "\n",
    "    lables_float = labels.astype(float)\n",
    "\n",
    "    labels_mult = (lables_float * 10).astype(int)\n",
    "    labels_matrix = labels_mult.as_matrix()\n",
    "\n",
    "    return labels_matrix\n",
    "\n",
    "#%%\n",
    "################################### Run Steps ########################################################################\n",
    "def main():\n",
    "\n",
    "    # Load the Trainign data\n",
    "    all_data = read_data(path)\n",
    "    #%%\n",
    "    training_data = pd.read_csv(train_data_path, encoding='iso-8859-1')\n",
    "\n",
    "    # use the below to split the training, validation and test\n",
    "    train_df = training_data_split(training_data)\n",
    "    #%%\n",
    "\n",
    "    # Load glove vector\n",
    "    glove_model = filter_glove(gloveFile)\n",
    "\n",
    "    # Get glove vector subset for training vocab\n",
    "    vocab_df = word_vec_index(all_data, glove_model)\n",
    "    glove_model = None\n",
    "\n",
    "    #Run this after the first iteration of obtaining the vocab df instead of above 2 steps\n",
    "    vocab_df = pd.read_csv(vocab_path, encoding='iso-8859-1')\n",
    "\n",
    "    #Get Wordlist and word vec lists from the df for the training Vocab\n",
    "    wordList, wordVectors = word_list(vocab_df)\n",
    "    wordVectors_df = pd.DataFrame(wordVectors)\n",
    "    wordVectors_df.to_csv(wordVectors_path)\n",
    "\n",
    "    # get the index of the word vec for each sentences to be input to the tf algo\n",
    "    max_seq_len, avg_len, sequence_length = maxSeqLen(training_data)\n",
    "    sequence_length_df = pd.DataFrame(sequence_length)\n",
    "    sequence_length_df.to_csv(sequence_len_path)\n",
    "\n",
    "    # training data input matrix\n",
    "    sentence_matrix = tf_data_pipeline(training_data, wordList, wordVectors, max_seq_len)\n",
    "\n",
    "    # export the sentence matrix to a csv file for easy load for next iterations\n",
    "    sentence_matrix_df = pd.DataFrame(sentence_matrix)\n",
    "    sentence_matrix_df.to_csv(sent_matrix_path)\n",
    "\n",
    "    #################################################################### validation data set ############################################################\n",
    "    # load validation data\n",
    "    val_data = pd.read_csv(val_data_path, encoding='iso-8859-1')\n",
    "\n",
    "    # load glove model and generat vocab for validation data\n",
    "    glove_model = loadGloveModel(gloveFile)\n",
    "    vocab_df_val = word_vec_index(val_data, glove_model)\n",
    "    glove_model = None\n",
    "    wordList_val, wordVectors_val = word_list(vocab_df_val)\n",
    "\n",
    "    # get max length for val data\n",
    "    max_seq_len_val, avg_len_val, sequence_length_val = maxSeqLen(val_data)\n",
    "    sequence_length_val_df = pd.DataFrame(sequence_length_val)\n",
    "    sequence_length_val_df.to_csv(sequence_len_val_path)\n",
    "\n",
    "    # get the id matrix for val data\n",
    "    sentence_matrix_val = tf_data_pipeline(val_data, wordList_val, wordVectors_val, max_seq_len)\n",
    "\n",
    "    # write the val dat to csv\n",
    "    sentence_matrix_df_val = pd.DataFrame(sentence_matrix_val)\n",
    "    sentence_matrix_df_val.to_csv(sent_matrix_path_val)\n",
    "\n",
    "    #################################################################### Test data set ############################################################\n",
    "    # load test data\n",
    "    test_data = pd.read_csv(test_data_path, encoding='iso-8859-1')\n",
    "\n",
    "    # load glove model and generat vocab for test data\n",
    "    glove_model = loadGloveModel(gloveFile)\n",
    "    vocab_df_test = word_vec_index(val_data, glove_model)\n",
    "    glove_model = None\n",
    "    wordList_test, wordVectors_test = word_list(vocab_df_test)\n",
    "\n",
    "    # get max length for test data\n",
    "    max_seq_len_test, avg_len_test, sequence_length_test = maxSeqLen(test_data)\n",
    "    sequence_length_test_df = pd.DataFrame(sequence_length_test)\n",
    "    sequence_length_test_df.to_csv(sequence_len_test_path)\n",
    "\n",
    "    # get the id matrix for test data\n",
    "    sentence_matrix_test = tf_data_pipeline(test_data, wordList_test, wordVectors_test, max_seq_len_test)\n",
    "\n",
    "    # write the test dat to csv\n",
    "    sentence_matrix_df_test= pd.DataFrame(sentence_matrix_test)\n",
    "    sentence_matrix_df_test.to_csv(sent_matrix_path_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
